{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation of a paragraph into sentence or a sentence into words is called tokenisation.\n",
    "\n",
    "There are various ways to do it:\n",
    "\n",
    "\n",
    "Stemming: Converting words in a sentence into base words. for example, eating to eat, loves to love\n",
    "* It Removes suffixes from the end of words to find their base form, or \"stem\". Stemming is quick and efficient, and is useful for processing large amounts of text. However, it can be crude and sometimes lead to unmeaningful base roots.\n",
    "\n",
    "\n",
    "Lemmatization: Converting words in a sentence into root words. for example, ate to eat, better to best\n",
    "* It analyzes the context of a sentence to reduce a word to its root form, or \"lemma\". Lemmatization is more accurate than stemming at finding meaningful dictionary words. However, it's more complex and computationally intensive than stemming."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
